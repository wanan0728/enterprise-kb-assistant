
1. 
sudo apt update

2. 
sudo apt install ca-certificates curl

3. 
sudo install -m 0755 -d /etc/apt/keyrings

4. 
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc

5. 
sudo chmod a+r /etc/apt/keyrings/docker.asc

6. 
sudo tee /etc/apt/sources.list.d/docker.sources <<EOF
Types: deb
URIs: https://download.docker.com/linux/ubuntu
Suites: $(. /etc/os-release && echo "${UBUNTU_CODENAME:-$VERSION_CODENAME}")
Components: stable
Signed-By: /etc/apt/keyrings/docker.asc
EOF

7. 
sudo apt update

8. 
sudo apt install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin


docker虚拟机，虚拟各种软件
dockdr虚拟的每一个软件一定是很小/迷你linux+这个软件的最小闭环
chromadb、mysql、redis

sudo docker ps      查看正在运行的容器（软件）
sudo docker ps -a   查看所有容器（软件）——包括那些停止的

sudo docker start 容器编号   # 启动一个已经停止的容器


sudo docker network create some-network

第一次准备mysql
sudo docker run --env=MYSQL_ROOT_PASSWORD=123456  --network=some-network -p 3306:3306 --restart=always  -d mysql:8

第一次准备chromada
docker run -d --name chroma --restart=always -p 8000:8000 -e IS_PERSISTENT=TRUE -e PERSIST_DIRECTORY=/chroma/chroma chromadb/chroma

下面是为了用自己的mysql用户和数据库——避免root
语法：
CREATE DATABASE 数据库名 CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;

CREATE USER '用户名'@'%' IDENTIFIED BY '密码';

GRANT alter,select,insert,update,delete,create,drop,index,references,CREATE VIEW,TRIGGER,show view, ALTER ROUTINE, create routine, execute, create temporary tables ON 数据库名.* TO '用户名'@'%';

例子：
CREATE DATABASE enterprise_kb CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;

CREATE USER 'tom'@'%' IDENTIFIED BY '123456';

GRANT alter,select,insert,update,delete,create,drop,index,references,CREATE VIEW,TRIGGER,show view, ALTER ROUTINE, create routine, execute, create temporary tables ON enterprise_kb.* TO 'tom'@'%';


我们这个项目需要2个模型：大语言模型，向量/文本嵌入模型。
大语言模型deepseek，向量/文本嵌入模型qianwen

1. app/config.py
from pydantic import BaseModel
from dotenv import load_dotenv
import os

load_dotenv()

class Settings(BaseModel):    
    openai_api_key: str = os.getenv("OPENAI_API_KEY", "")
    model_name: str = os.getenv("MODEL_NAME", "gpt-4o-mini")
    chroma_dir: str = os.getenv("CHROMA_DIR", "./data/chroma")
    chroma_host: str = os.getenv("CHROMA_HOST", "localhost")
    chroma_port: int = int(os.getenv("CHROMA_PORT", "8000"))
    collection_name: str = os.getenv("COLLECTION_NAME", "knowledge_base")
    chunk_size: int = int(os.getenv("CHUNK_SIZE", "800"))
    chunk_overlap: int = int(os.getenv("CHUNK_OVERLAP", "120"))

settings = Settings()


2. app/rag/vectorstore.py  

import chromadb
from langchain_chroma import Chroma  # langchain和chromedb结合需要使用的依赖
from app.config import settings

def get_vectorstore(embeddings): 
    client = chromadb.HttpClient(
        host=settings.chroma_host,
        port=settings.chroma_port
    )  # client表示获得一个指向chromadb的连接
    return Chroma(
        client=client,
        collection_name=settings.collection_name,
        embedding_function=embeddings,
    )

3. app/deps.py 
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from app.config import settings
from app.rag.vectorstore import get_vectorstore

def get_llm():
    return ChatOpenAI(
        model=settings.model_name,
        api_key=settings.openai_api_key,
        temperature=0.2,
        streaming=True,
    )

def get_embeddings():
    return OpenAIEmbeddings(api_key=settings.openai_api_key)

def get_vs():
    return get_vectorstore(get_embeddings())

4. app/ingestion/loader.py
from pathlib import Path
from typing import List
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pypdf import PdfReader
import docx
from app.config import settings

def load_pdf(path: Path) -> List[Document]:
	"""下面的代码将pdf分割成单独的页面，每一个页面的文本被封装成一个Document放入list"""
    reader = PdfReader(str(path))
    docs = []
    for i, page in enumerate(reader.pages):
        text = page.extract_text() or ""
        if text.strip():
            docs.append(Document(
                page_content=text,
                metadata={"source": str(path), "page": i+1}
            ))
    return docs

def load_docx(path: Path) -> List[Document]:
    d = docx.Document(str(path))
    text = "\n".join(p.text for p in d.paragraphs if p.text.strip())
    return [Document(page_content=text, metadata={"source": str(path)})] if text else []

def load_docs(dir_path: str) -> List[Document]:
    p = Path(dir_path)
    docs: List[Document] = []
    for f in p.rglob("*"):
        if f.suffix.lower() == ".pdf":
            docs.extend(load_pdf(f))
        elif f.suffix.lower() in [".docx", ".doc"]:
            docs.extend(load_docx(f))
        elif f.suffix.lower() in [".md", ".txt"]:
            docs.append(Document(page_content=f.read_text(encoding="utf-8"),
                                 metadata={"source": str(f)}))
    return docs

def split_docs(docs: List[Document]) -> List[Document]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=settings.chunk_size,
        chunk_overlap=settings.chunk_overlap
    )
    return splitter.split_documents(docs)


5. app/ingestion/build_index.py
from app.ingestion.loader import load_docs, split_docs
from app.deps import get_embeddings, get_vs

def main():
    docs = split_docs(load_docs("./data/docs"))
    vs = get_vs()
    vs.add_documents(docs)
	try:
	    vs.persist()
	except Exception:
	    pass
	print(f"Indexed {len(docs)} chunks into Chroma.")
if __name__ == "__main__":
	main()  # 为啥这里放一个main，主要我们打算用来做脚本，单元测试好，但是脚本也要随时运行，所以我放了一个main在里面，随时python -m app.ingestion.build_index



用自然语言和大语言模型沟通，让它去向量数据库中检索文档，大语言模型再将结果整理后返回给我们。
1. 提示词的书写
2. langgraph：工作流


6. app/rag/prompts.py

QA_SYSTEM = """你是企业知识助理。
- 严格基于给定证据回答，不能编造。
- 若证据不足，说明缺口并提出澄清问题或建议提交工单。
- 输出必须包含引用编号。"""


QA_USER = """问题：{question}

证据（每条带编号）：
{context}

请基于证据回答，并在相关句末标注引用，如[1][3]。"""


7. app/rag/qa_graph.py
这个代码是第一个langgraph程序，需要大家知道的有：
1. 状态：状态就是一个对象，在各个节点之间传递.
这个对象一般都是在创建图的时候被内部创建。

2. 节点：节点就是函数，表示工作流中的某个功能，节点的参数必须是状态
节点的返回值是一个字典，字典的键就是状态中的某个或者某几个变量
用来表示这个节点对这个状态的修改

3. 条件判断：条件判断也是函数
他们的参数通常也是状态，但是返回值一定是str
条件判断返回的str决定了下一次要去哪个节点

4. 构造图

from typing import TypedDict, List, Any

from langgraph.graph import StateGraph, START, END
from langchain_core.messages import HumanMessage, AIMessage

from app.rag.prompts import QA_SYSTEM, QA_USER
from app.deps import get_llm, get_vs

"""下面是LangGraph中的状态结构（类似上下文）：
• question: 用户问的问题
• user_role: 用户角色，例如employee, hr, admin，用来做权限控制
• docs: 检索到的文档列表（LangChain的Document）
• answer: 最终给用户的回答
• messages: 预留的消息列表（目前代码里没用到，但以后可以用来存对话历史）
整个图在运行时就一直在改这一个字典"""

class QAState(TypedDict, total=False):
    question: str
    text: str          # 兼容 /chat 传 text
    user_role: str
    docs: List[Any]
    answer: str
    messages: List[Any]

def decide_retrieve(state: QAState) -> str:
    return "retrieve"

def decide_retrieve_node(state: QAState) -> dict:
    """
    节点 runnable：必须返回 dict
    这里只是一个no-op节点，真正路由在decide_retrieve()里完成
    """
    return {}

def retrieve(state: QAState) -> dict:
    vs = get_vs()
    role = state.get("user_role", "public")
    query = state.get("question") or state.get("text") or ""

    retriever = vs.as_retriever(
        search_kwargs={
            "k": 8,
            "filter": {"visibility": {"$in": ["public", role]}},
        }
    )
    docs = retriever.invoke(query)

    if not docs:
        retriever2 = vs.as_retriever(search_kwargs={"k": 8})
        docs = retriever2.invoke(query)
        return {"docs": docs, "question": query, "debug": "fallback_unfiltered"}

    return {"docs": docs, "question": query, "debug": "filtered"}


def grade_evidence(state: QAState) -> str:
    """检索后判断是否有证据。"""
    return "good" if state.get("docs") else "bad"


def generate_answer(state: QAState) -> dict:
    """带引用生成答案。"""
    llm = get_llm()
    docs = state.get("docs", [])

    context = "\n\n".join(
        f"[{i+1}] {d.page_content}\n(source={d.metadata.get('source')}, page={d.metadata.get('page')})"
        for i, d in enumerate(docs[:6])
    )

    prompt = QA_USER.format(question=state["question"], context=context)
    messages = [AIMessage(content=QA_SYSTEM), HumanMessage(content=prompt)]
    ans = llm.invoke(messages).content
    return {"answer": ans}


def refuse_or_clarify(state: QAState) -> dict:
    """无证据兜底。"""
    return {
        "answer": "我没有在当前可见知识库中找到足够证据回答。请提供更具体的关键词/文档来源。"
    }


def build_qa_graph():
    g = StateGraph(QAState)

    # 注意：节点注册用 runnable（返回 dict）
    g.add_node("decide_retrieve", decide_retrieve_node)
    g.add_node("retrieve", retrieve)
    g.add_node("generate", generate_answer)
    g.add_node("refuse", refuse_or_clarify)

    g.add_edge(START, "decide_retrieve")

    g.add_conditional_edges(
        "decide_retrieve",
        decide_retrieve,
        {
            "retrieve": "retrieve",
            "direct": "generate",
        },
    )

    g.add_conditional_edges(
        "retrieve",
        grade_evidence,
        {
            "good": "generate",
            "bad": "refuse",
        },
    )

    g.add_edge("generate", END)
    g.add_edge("refuse", END)

    return g.compile()









