Python看开头和最后几行

rag是一种思想或者说是论文

将文档向量化，放入向量数据库中
方便大模型进行检索


用嵌入式模型将文档向量化。

1. app/rag/vectorstore.py
这个文件的主要作用是对chromada进行配置
import chromadb
from langchain_chroma import Chroma
from app.config import settings

def get_vectorstore(embeddings):
    # Connect to Chroma Server running in Docker (chromadb/chroma:0.6.3)
    client = chromadb.HttpClient(
        host=settings.chroma_host,
        port=settings.chroma_port
    )
    return Chroma(  # 返回的就是一个指向chromadb的连接
        client=client,
        collection_name=settings.collection_name,  # 为我们这个项目建议一个专属的数据库
        embedding_function=embeddings,
    )


2. 修改depts.py
from app.rag.vectorstore import get_vectorstore

添加如下方法
def get_vs():
    return get_vectorstore(get_embeddings())


3. 准备word、pdf、txt等文件放入根目录下的data/docs文件夹

4. app/rag/prompts.py 提示词

QA_SYSTEM = """你是企业知识助理。
- 严格基于给定证据回答，不能编造。
- 若证据不足，说明缺口并提出澄清问题或建议提交工单。
- 输出必须包含引用编号。"""

QA_USER = """问题：{question}

证据（每条带编号）：
{context}

请基于证据回答，并在相关句末标注引用，如[1][3]。"""


5. app/ingestion/loader.py
下面的代码从一个目录下递归加载各种文档文件pdf/docx/doc/md/txt等，将它们读成LangChain的 Document对象列表。然后用RecursiveCharacterTextSplitter把这些文档按在settings里配置的
chunk_size和chunk_overlap切成小块，方便后面做向量化/检索。

chunk表示切出来的一小块文本。

chunk_size是每个文本块的最大长度（按字符数算）。
块太大: 
	向量长度大，embedding变慢、占空间
	模型一次看太多文本，反而不精细
块太小: 
	语义被切碎，一块的信息太少
	需要更多块才能覆盖同一篇文档

典型取值512~1500字符，如果要语义更完整1000左右比较常见
如果文档都特别长想加速处理可以用1500–2000

chunk_overlap是相邻两个chunk之间的重叠字符数。
比如chunk_size = 1000，chunk_overlap = 200

切出来是这样的
第1块：0 ~ 999
第2块：800 ~ 1799
第3块：1600 ~ 2599

为什么要重叠？
文本语义经常跨chunk，比如：段落结尾一句话 + 下一段开头一起才好理解
如果没有重叠，检索某些句子时，可能刚好被切开，导致模型missing context
有重叠可以让模型在任意块里都看到一部分上下文

一般是chunk_size的 10%～30%
如果取值太大如800会：增加很多重复内容、向量库变大，检索变慢

from pathlib import Path
from typing import List
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pypdf import PdfReader
import docx
from app.config import settings

def load_pdf(path: Path) -> List[Document]:
	"""下面的代码将pdf分割成单独的页面，每一个页面的文本被封装成一个Document放入list"""
    reader = PdfReader(str(path))
    docs = []
    for i, page in enumerate(reader.pages):
        text = page.extract_text() or ""
        if text.strip():
            docs.append(Document(
                page_content=text,
                metadata={"source": str(path), "page": i+1}
            ))
    return docs

def load_docx(path: Path) -> List[Document]:
    d = docx.Document(str(path))
    text = "\n".join(p.text for p in d.paragraphs if p.text.strip())
    return [Document(page_content=text, metadata={"source": str(path)})] if text else []

def load_docs(dir_path: str) -> List[Document]:
    p = Path(dir_path)
    docs: List[Document] = []
    for f in p.rglob("*"):
        if f.suffix.lower() == ".pdf":
            docs.extend(load_pdf(f))
        elif f.suffix.lower() in [".docx", ".doc"]:
            docs.extend(load_docx(f))
        elif f.suffix.lower() in [".md", ".txt"]:
            docs.append(Document(page_content=f.read_text(encoding="utf-8"),
                                 metadata={"source": str(f)}))
    return docs

def split_docs(docs: List[Document]) -> List[Document]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=settings.chunk_size,
        chunk_overlap=settings.chunk_overlap
    )
    return splitter.split_documents(docs)


6. app/ingestion/build_index.py
执行下面代码之前，准备好向量数据库。
docker run -d \
  --name chroma \
  -p 8000:8000 \
  -e IS_PERSISTENT=TRUE \
  -e PERSIST_DIRECTORY=/chroma/chroma \
  chromadb/chroma

利用上一个环节的代码将切碎后的文档写入chroma
from app.ingestion.loader import load_docs, split_docs
from app.deps import get_embeddings, get_vs

def main():
    docs = split_docs(load_docs("./data/docs"))
    get_vs().add_documents(docs)
	print(f"Indexed {len(docs)} chunks into Chroma.")

if __name__ == "__main__":
	main()  

为了执行上面的代码，必须在项目的根目录下打开终端，输入
python -m app.ingestion.build_index



每天要总结，总结的作用是记录每次做了什么内容。好放在简历的里面。
1. 学会了docker中搭建向量数据库
2. 学会了代码中如何配置向量数据库和嵌入式模型
3. 学会了利用docx和pypdf两个库读取word和pdf文件
4. 如何遍历某个文件夹找到其中的所有文件
5. 切割文件为小的chunk（向量数据库中的每一个小块）
6. 如何将chunks放入向量数据库等待检索

项目总体介绍
3行

我的职责
1. 嵌入式模型切块放入向量数据库并检索

7.



















