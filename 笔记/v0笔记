app/ingestion/：文档加载、切分、建Chroma索引脚本
app/rag/：Agentic RAG QA子图（带引用、低证据拒答）
app/workflows/：工单Action子图（抽槽→规则校验→创建工单mock）
app/router_graph.py：顶层LangGraph路由
app/main.py：FastAPIchat接口
data/docs/：企业文档
data/chroma/：本地向量持久化目录


1. 首先确定配置app/config.py
from pydantic import BaseModel
from dotenv import load_dotenv
import os

load_dotenv()

class Settings(BaseModel):
    openai_api_key: str = os.getenv("OPENAI_API_KEY", "")
    model_name: str = os.getenv("MODEL_NAME", "gpt-4o-mini")
    chroma_dir: str = os.getenv("CHROMA_DIR", "./data/chroma")
    chroma_host: str = os.getenv("CHROMA_HOST", "localhost")
    chroma_port: int = int(os.getenv("CHROMA_PORT", "8000"))
    collection_name: str = os.getenv("COLLECTION_NAME", "knowledge_base")
    chunk_size: int = int(os.getenv("CHUNK_SIZE", "800"))
    chunk_overlap: int = int(os.getenv("CHUNK_OVERLAP", "120"))

settings = Settings()


2. app/rag/vectorstore.py  配置chroma
先启动向量数据库
docker run -d \
  --name chroma \
  -p 8000:8000 \
  -e IS_PERSISTENT=TRUE \
  -e PERSIST_DIRECTORY=/chroma/chroma \
  chromadb/chroma

import chromadb
from langchain_chroma import Chroma  # langchain和chromedb结合需要使用的依赖
from app.config import settings

def get_vectorstore(embeddings): 
    client = chromadb.HttpClient(
        host=settings.chroma_host,
        port=settings.chroma_port
    )  # client表示获得一个指向chromadb的连接
    return Chroma(
        client=client,
        collection_name=settings.collection_name,
        embedding_function=embeddings,
    )

3. app/deps.py  配置大模型
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from app.config import settings
from app.rag.vectorstore import get_vectorstore

def get_llm():
    return ChatOpenAI(
        model=settings.model_name,
        api_key=settings.openai_api_key,
        temperature=0.2,
        streaming=True,
    )

def get_embeddings():
    return OpenAIEmbeddings(api_key=settings.openai_api_key)

def get_vs():
    return get_vectorstore(get_embeddings())

if __name__ == '__main__':
    print('--------------')
    print(get_llm())
    print('--------------')
    print(get_embeddings())
    print('--------------')
    print(get_vs())
    print('--------------')


4. app/ingestion/loader.py
下面的代码从一个目录下递归加载各种文档文件pdf/docx/doc/md/txt等，将它们读成LangChain的 Document对象列表。然后用RecursiveCharacterTextSplitter把这些文档按在settings里配置的
chunk_size和chunk_overlap切成小块，方便后面做向量化/检索。

chunk表示切出来的一小块文本。

chunk_size是每个文本块的最大长度（按字符数算）。
块太大: 
	向量长度大，embedding变慢、占空间
	模型一次看太多文本，反而不精细
块太小: 
	语义被切碎，一块的信息太少
	需要更多块才能覆盖同一篇文档

典型取值512~1500字符，如果要语义更完整1000左右比较常见
如果文档都特别长想加速处理可以用1500–2000

chunk_overlap是相邻两个chunk之间的重叠字符数。
比如chunk_size = 1000，chunk_overlap = 200

切出来是这样的
第1块：0 ~ 999
第2块：800 ~ 1799
第3块：1600 ~ 2599

为什么要重叠？
文本语义经常跨chunk，比如：段落结尾一句话 + 下一段开头一起才好理解
如果没有重叠，检索某些句子时，可能刚好被切开，导致模型missing context
有重叠可以让模型在任意块里都看到一部分上下文

一般是chunk_size的 10%～30%
如果取值太大如800会：增加很多重复内容、向量库变大，检索变慢

from pathlib import Path
from typing import List
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pypdf import PdfReader
import docx
from app.config import settings

def load_pdf(path: Path) -> List[Document]:
	"""下面的代码将pdf分割成单独的页面，每一个页面的文本被封装成一个Document放入list"""
    reader = PdfReader(str(path))
    docs = []
    for i, page in enumerate(reader.pages):
        text = page.extract_text() or ""
        if text.strip():
            docs.append(Document(
                page_content=text,
                metadata={"source": str(path), "page": i+1}
            ))
    return docs

def load_docx(path: Path) -> List[Document]:
    d = docx.Document(str(path))
    text = "\n".join(p.text for p in d.paragraphs if p.text.strip())
    return [Document(page_content=text, metadata={"source": str(path)})] if text else []

def load_docs(dir_path: str) -> List[Document]:
    p = Path(dir_path)
    docs: List[Document] = []
    for f in p.rglob("*"):
        if f.suffix.lower() == ".pdf":
            docs.extend(load_pdf(f))
        elif f.suffix.lower() in [".docx", ".doc"]:
            docs.extend(load_docx(f))
        elif f.suffix.lower() in [".md", ".txt"]:
            docs.append(Document(page_content=f.read_text(encoding="utf-8"),
                                 metadata={"source": str(f)}))
    return docs

def split_docs(docs: List[Document]) -> List[Document]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=settings.chunk_size,
        chunk_overlap=settings.chunk_overlap
    )
    return splitter.split_documents(docs)


5. app/ingestion/build_index.py
利用上一个环节的代码将文档切碎后写入chroma
from app.ingestion.loader import load_docs, split_docs
from app.deps import get_embeddings, get_vs

def main():
    docs = split_docs(load_docs("./data/docs"))
    vs = get_vs()
    vs.add_documents(docs)
	try:
	    vs.persist()
	except Exception:
	    pass
	print(f"Indexed {len(docs)} chunks into Chroma.")
if __name__ == "__main__":
	main()  # 为啥这里放一个main，主要我们打算用来做脚本，单元测试好，但是脚本也要随时运行，所以我放了一个main在里面，随时python -m app.ingestion.build_index



6. app/rag/prompts.py
QA_SYSTEM = """你是企业知识助理。
- 严格基于给定证据回答，不能编造。
- 若证据不足，说明缺口并提出澄清问题或建议提交工单。
- 输出必须包含引用编号。"""

QA_USER = """问题：{question}

证据（每条带编号）：
{context}

请基于证据回答，并在相关句末标注引用，如[1][3]。"""


7. app/rag/qa_graph.py
这段代码就是用LangGraph搭了一个简易的问答工作流，这个工作流可以概括为：

用户提问 -> 决定要不要检索）→ 去Chroma里按权限搜文档 → 判断有没有找到证据 → 有就用LLM带引用生成答案 → 没有就礼貌拒答/让用户补充信息。

整个流程被拆成几个节点node，再用状态机StateGraph把节点串起来。

from typing import TypedDict, List, Any

from langgraph.graph import StateGraph, START, END
from langchain_core.messages import HumanMessage, AIMessage

from app.rag.prompts import QA_SYSTEM, QA_USER
from app.deps import get_llm, get_vs

"""下面是LangGraph中的状态结构（类似上下文）：
• question: 用户问的问题
• user_role: 用户角色，例如employee, hr, admin，用来做权限控制
• docs: 检索到的文档列表（LangChain的Document）
• answer: 最终给用户的回答
• messages: 预留的消息列表（目前代码里没用到，但以后可以用来存对话历史）
整个图在运行时就一直在改这一个字典"""

class QAState(TypedDict, total=False):
    question: str
    text: str          # 兼容 /chat 传 text
    user_role: str
    docs: List[Any]
    answer: str
    messages: List[Any]

def decide_retrieve(state: QAState) -> str:
    return "retrieve"

def decide_retrieve_node(state: QAState) -> dict:
    """
    节点 runnable：必须返回 dict
    这里只是一个no-op节点，真正路由在decide_retrieve()里完成
    """
    return {}

def retrieve(state: QAState) -> dict:
    vs = get_vs()
    role = state.get("user_role", "public")
    query = state.get("question") or state.get("text") or ""

    retriever = vs.as_retriever(
        search_kwargs={
            "k": 8,
            "filter": {"visibility": {"$in": ["public", role]}},
        }
    )
    docs = retriever.invoke(query)

    if not docs:
        retriever2 = vs.as_retriever(search_kwargs={"k": 8})
        docs = retriever2.invoke(query)
        return {"docs": docs, "question": query, "debug": "fallback_unfiltered"}

    return {"docs": docs, "question": query, "debug": "filtered"}


def grade_evidence(state: QAState) -> str:
    """检索后判断是否有证据。"""
    return "good" if state.get("docs") else "bad"


def generate_answer(state: QAState) -> dict:
    """带引用生成答案。"""
    llm = get_llm()
    docs = state.get("docs", [])

    context = "\n\n".join(
        f"[{i+1}] {d.page_content}\n(source={d.metadata.get('source')}, page={d.metadata.get('page')})"
        for i, d in enumerate(docs[:6])
    )

    prompt = QA_USER.format(question=state["question"], context=context)
    messages = [AIMessage(content=QA_SYSTEM), HumanMessage(content=prompt)]
    ans = llm.invoke(messages).content
    return {"answer": ans}


def refuse_or_clarify(state: QAState) -> dict:
    """无证据兜底。"""
    return {
        "answer": "我没有在当前可见知识库中找到足够证据回答。请提供更具体的关键词/文档来源。"
    }


def build_qa_graph():
    g = StateGraph(QAState)

    # 注意：节点注册用 runnable（返回 dict）
    g.add_node("decide_retrieve", decide_retrieve_node)
    g.add_node("retrieve", retrieve)
    g.add_node("generate", generate_answer)
    g.add_node("refuse", refuse_or_clarify)

    g.add_edge(START, "decide_retrieve")

    g.add_conditional_edges(
        "decide_retrieve",
        decide_retrieve,
        {
            "retrieve": "retrieve",
            "direct": "generate",
        },
    )

    g.add_conditional_edges(
        "retrieve",
        grade_evidence,
        {
            "good": "generate",
            "bad": "refuse",
        },
    )

    g.add_edge("generate", END)
    g.add_edge("refuse", END)

    return g.compile()


8. app/workflows/models.py（工单示例）
from pydantic import BaseModel
from typing import Optional, Literal

class TicketCreate(BaseModel):
    """pydantic是一个用类型注解来做数据校验和转换的库
    这里就是一个数据模型，然后它加了一点校验规则而已"""
    category: Literal["it", "hr", "finance"] = "it"  # 分类
    priority: Literal["P0","P1","P2","P3"] = "P2"  # 优先级
    location: str  # 必须的字段，位置，共担创建的位置，比如背景总部，上海分公司等
    description: str
    requester: str  # 必须的字段，谁提的这个单子
    attachment_url: Optional[str] = None  # 可选字段，此处是附件的意思



9. app/workflows/rules.py
# 在Pydantic把数据格式校验完之后再做一层业务规则校验

from app.workflows.models import TicketCreate
def validate_ticket(t: TicketCreate, user_role: str):
	# 参数t表示已经被Pydantic教研过的工单数据，字段类型都没啥问题
    if t.priority == "P0" and user_role not in ["manager", "it_admin"]:
        return False, "只有经理或IT管理员可以提交P0工单。"
    if t.category == "it" and "照片" in t.description and not t.attachment_url:
        return False, "IT设备类问题建议附带现场照片。"
    return True, ""


10. app/workflows/tools.py（先做本地 mock）
# 先不接真数据库，而是用一个内存里的dict假装是db把CRUD流程跑通
import uuid
from typing import Dict, Any
from app.workflows.models import TicketCreate

_FAKE_DB: Dict[str, Any] = {}

def create_ticket(t: TicketCreate) -> Dict[str, Any]:
	# 生成工单并存到假db里
    tid = str(uuid.uuid4())
    data = t.model_dump()  # pydantic模型专程dict
    data.update({"id": tid, "status": "NEW"})  # 给这个字典加点东西
    _FAKE_DB[tid] = data
    return data

def get_ticket(tid: str):
    return _FAKE_DB.get(tid)


10. app/workflows/action_graph.py
# 这套ActionState图就是用户随便打一段话 -> LLM抽取结构化字段(slots) -> Pydantic + 自定义规则校验 -> 创建工单 -> 返回一句确认话术
from typing import TypedDict
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage
from pydantic import ValidationError
from app.deps import get_llm
from app.workflows.models import TicketCreate
from app.workflows.rules import validate_ticket
from app.workflows.tools import create_ticket

class ActionState(TypedDict):
    text: str  # 用户输入的句子
    user_role: str  # 橘色
    requester: str  # 提交者
    slots: dict  # LLM抽出来的字段
    ticket: dict  #最终构建好的工单数据
    answer: str   # 返回给用户的一句话，比如错了，成功等

EXTRACT_PROMPT = """你是企业工单助手。
从用户文本中抽取工单字段，输出 JSON。
字段: category(it/hr/finance), priority(P0-P3), location, description.
用户文本: {text}"""

def extract_slots(state: ActionState):
	# 第一个节点，LLM抽槽位，就是让llm从一段乱七八糟的自然语言里，把我们关心的几个结构化字段给抠出来装进一个固定的表格里
    llm = get_llm()
    content = llm.invoke([HumanMessage(content=EXTRACT_PROMPT.format(text=state["text"]))]).content
    import json
    slots = json.loads(content)
    return {"slots": slots}

def validate_and_build(state: ActionState):
    try:
        t = TicketCreate(**state["slots"], requester=state["requester"])
    except ValidationError as e:
        return {"answer": f"字段不完整或格式不对：{e}. 请补充信息。"}
    ok, msg = validate_ticket(t, state["user_role"])
    if not ok:
        return {"answer": msg}
    return {"ticket": t.model_dump()}  # 把Pydantic模型t dump成干净的dict

def user_confirm(state: ActionState):
    return "confirm"  # 需要用户确认一下，这里就是站位

def do_create(state: ActionState):  # 真正创建工单外加回答
    t = TicketCreate(**state["ticket"])
    created = create_ticket(t)
    return {"answer": f"已为你创建工单：{created['id']}，优先级 {created['priority']}，地点 {created['location']}。"}

def build_action_graph():  # 和上一个例子一样，不讲了，自己研究下
    g = StateGraph(ActionState)
    g.add_node("extract", extract_slots)
    g.add_node("validate", validate_and_build)
    g.add_node("create", do_create)

    g.set_entry_point("extract")
    g.add_edge("extract", "validate")
    g.add_conditional_edges("validate", user_confirm, {"confirm": "create"})
    g.add_edge("create", END)
    return g.compile()


12. app/router_graph.py（顶层路由）
我们这里搭了一个顶层Router图，
这个非常简单，现在只有一种模式：不管怎样都把请求丢给qa_graph去做RAG。

后面我们可以在这里挂更多模式，比如：
• action -> 走前面那个工单创建workflow
• chat -> 走一个纯闲聊LLM
• tool -> 调用别的工具

现在是一个最小可用版本，只实现了Q&A路由
from typing import TypedDict, Any
from langgraph.graph import StateGraph, START, END
from app.rag.qa_graph import build_qa_graph

class RouterState(TypedDict, total=False):  # 顶层状态结构，total=False表示下面所有字段都是可选的
    question: str  # 给QA的问题
    text: str  # 用户原始文本
    user_role: str  # 用户角色
    mode: str  # 模式标记，比如qa，rag，kb等等
    answer: str  # 答案
    docs: list[Any]  # QA检索到的文档列表

def decide_route(state: RouterState) -> str:
    return "qa" # 这个代码反正最后都是qa


def route_node(state: RouterState) -> dict:
	# 路由前的站位节点，返回{}意味着不修改状态
    return {}


def build_router_graph():
    g = StateGraph(RouterState)

    qa_graph = build_qa_graph()  # 刚才的子图

    g.add_node("route", route_node)
    g.add_node("qa", qa_graph)

    g.add_edge(START, "route")

    g.add_conditional_edges(
        "route", decide_route,
        {"qa": "qa"},
    )

    g.add_edge("qa", END)

    return g.compile()

router_graph = build_router_graph()


13. app/main.py（FastAPI）
from fastapi import FastAPI
from pydantic import BaseModel
from app.router_graph import router_graph

app = FastAPI(title="Enterprise KB Assistant")

class ChatReq(BaseModel):
    text: str
    user_role: str = "public"
    requester: str = "anonymous"

class ChatResp(BaseModel):
    answer: str

@app.post("/chat", response_model=ChatResp)
def chat(req: ChatReq):
    out = router_graph.invoke(req.model_dump())
    return {"answer": out["answer"]}


14. 测试
python -m app.ingestion.build_inde
uvicorn app.main:app --reload --port 8002

curl -X POST http://127.0.0.1:8002/chat \
  -H "Content-Type: application/json" \
  -d '{"text":"年假怎么申请？","user_role":"public"}'

测试的时候如果出现Internal Server Error错误，立刻去代码里看输出，一定是Python代码报错了。


{"answer":"申请年假的流程如下：\n\n1. **提前申请**：员工需至少提前1个工作日提交请假申请（病假可事后补提）。\n2. **系统校验**：系统会自动校验剩余额度与冲突日程；如存在冲突，员工需与直属主管沟通后调整。\n3. **审批流程**：\n   - 请假时间在0.5–2天的，由直属主管审批。\n   - 请假时间在2–5天的，需要直属主管和部门负责人审批。\n   - 请假时间超过5天的，需经过以上审批并由HR备案。\n\n4. **取消与变更**：请假开始前可在系统中撤回或改期；开始后需由主管确认变更原因。对已批准年假进行变更的，系统会自动回补或扣减对应年假余额[1]。\n\n如果您有其他具体问题或需要进一步的帮助，请告知！"}%                                    (base) peter@Peters-MacBook-Air ~ % 
