我们这个项目需要2个模型：大语言模型，向量/文本嵌入模型。
大语言模型deepseek，向量/文本嵌入模型qianwen


1. app/config.py
from pydantic import BaseModel
from dotenv import load_dotenv
import os

load_dotenv()

class Settings(BaseModel):
    openai_api_key: str = os.getenv("OPENAI_API_KEY", "")
    model_name: str = os.getenv("MODEL_NAME", "gpt-4o-mini")
    chroma_dir: str = os.getenv("CHROMA_DIR", "./data/chroma")
    chroma_host: str = os.getenv("CHROMA_HOST", "localhost")
    chroma_port: int = int(os.getenv("CHROMA_PORT", "8000"))
    collection_name: str = os.getenv("COLLECTION_NAME", "knowledge_base")
    chunk_size: int = int(os.getenv("CHUNK_SIZE", "800"))
    chunk_overlap: int = int(os.getenv("CHUNK_OVERLAP", "120"))

settings = Settings()


2. app/rag/vectorstore.py

import chromadb
from langchain_chroma import Chroma  # langchain和chromedb结合需要使用的依赖
from app.config import settings

def get_vectorstore(embeddings):
    client = chromadb.HttpClient(
        host=settings.chroma_host,
        port=settings.chroma_port
    )  # client表示获得一个指向chromadb的连接
    return Chroma(
        client=client,
        collection_name=settings.collection_name,
        embedding_function=embeddings,
    )


3. app/deps.py

from langchain_openai import ChatOpenAI
from app.rag.vectorstore import get_vectorstore
from langchain_community.embeddings import DashScopeEmbeddings
from app.config import settings

def get_llm():
    return ChatOpenAI(
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        model="qwen-flash",
        api_key="千问模型的key",
        temperature=0.2,
        streaming=True,
    )


def get_embeddings():
    return DashScopeEmbeddings(
        model="text-embedding-v3",
        dashscope_api_key="千问模型的key",
    )

def get_vs():
    return get_vectorstore(get_embeddings())

if __name__=='__main__':
    print(get_llm().invoke('你的版本是什么').content)


4. app/ingestion/loader.py

from pathlib import Path
from typing import List
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pypdf import PdfReader
import docx
from app.config import settings

def load_pdf(path: Path) -> List[Document]:
	"""下面的代码将pdf分割成单独的页面，每一个页面的文本被封装成一个Document放入list"""
    reader = PdfReader(str(path))
    docs = []
    for i, page in enumerate(reader.pages):
        text = page.extract_text() or ""
        if text.strip():
            docs.append(Document(
                page_content=text,
                metadata={"source": str(path), "page": i+1}
            ))
    return docs

def load_docx(path: Path) -> List[Document]:
    d = docx.Document(str(path))
    text = "\n".join(p.text for p in d.paragraphs if p.text.strip())
    return [Document(page_content=text, metadata={"source": str(path)})] if text else []

def load_docs(dir_path: str) -> List[Document]:
    p = Path(dir_path)
    docs: List[Document] = []
    for f in p.rglob("*"):
        if f.suffix.lower() == ".pdf":
            docs.extend(load_pdf(f))
        elif f.suffix.lower() in [".docx", ".doc"]:
            docs.extend(load_docx(f))
        elif f.suffix.lower() in [".md", ".txt"]:
            docs.append(Document(page_content=f.read_text(encoding="utf-8"),
                                 metadata={"source": str(f)}))
    return docs

def split_docs(docs: List[Document]) -> List[Document]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=settings.chunk_size,
        chunk_overlap=settings.chunk_overlap
    )
    return splitter.split_documents(docs)


5. app/ingestion/build_index.py

from app.ingestion.loader import load_docs, split_docs
from app.deps import get_embeddings, get_vs

def main():
    docs = split_docs(load_docs("./data/docs"))
    vs = get_vs()
    vs.add_documents(docs)
	try:
	    vs.persist()
	except Exception:
	    pass
	print(f"Indexed {len(docs)} chunks into Chroma.")
if __name__ == "__main__":
	main()  # 为啥这里放一个main，主要我们打算用来做脚本，单元测试好，但是脚本也要随时运行，所以我放了一个main在里面，随时python -m app.ingestion.build_index


6. app/rag/prompts.py

QA_SYSTEM = """你是企业知识助理。
- 严格基于给定证据回答，不能编造。
- 若证据不足，说明缺口并提出澄清问题或建议提交工单。
- 输出必须包含引用编号。"""


QA_USER = """问题：{question}

证据（每条带编号）：
{context}

请基于证据回答，并在相关句末标注引用，如[1][3]。"""


7. app/rag/qa_graph.py

这个代码是第一个langgraph程序，需要大家知道的有：
1. 状态：状态就是一个对象，在各个节点之间传递.
这个对象一般都是在创建图的时候被内部创建。

2. 节点：节点就是函数，表示工作流中的某个功能，节点的参数必须是状态
节点的返回值是一个字典，字典的键就是状态中的某个或者某几个变量
用来表示这个节点对这个状态的修改

from typing import TypedDict, List, Any

from langgraph.graph import StateGraph, START, END
from langchain_core.messages import HumanMessage, AIMessage

from app.rag.prompts import QA_SYSTEM, QA_USER
from app.deps import get_llm, get_vs

class QAState(TypedDict, total=False):
    question: str
    text: str          # 兼容 /chat 传 text
    user_role: str
    docs: List[Any]
    answer: str
    messages: List[Any]


def decide_retrieve(state: QAState) -> str:
    """
    条件函数：决定走检索还是直答
    返回值必须对应 add_conditional_edges 的 key
    """
    # 简化版：永远检索
    return "retrieve"  # 跳到下一个叫做retrieve的节点


def decide_retrieve_node(state: QAState) -> dict:
    """
    节点 runnable：必须返回 dict
    这里只是一个 no-op 节点，真正路由在 decide_retrieve() 里完成
    """
    return {} # 当一个节点返回空字典，表示什么也不做。


# ---------- retrieval / generation ----------

def retrieve(state: QAState) -> dict:
    """从 Chroma 检索相关文档。

    先按 visibility 做过滤；如果元数据里没有该字段导致检索为空，则回退到无过滤检索。
    """
    vs = get_vs()
    role = state.get("user_role", "public")
    query = state.get("question") or state.get("text") or ""

    # 1) filtered retrieval first
    retriever = vs.as_retriever(  # as_retriever用于构造一个要检索的需求
        search_kwargs={
            "k": 8,  # 最多查8个结果
            "filter": {"visibility": {"$in": ["public", role]}},
            # 只查询向量数据库中那些public的文档
        }
    )
    docs = retriever.invoke(query) # invoke就是真的去向量数据库查询
    # docs表示从向量数据库中查出来的文档

    # 2) fallback to unfiltered if empty (common when metadata doesn't contain `visibility`)
    if not docs: # 如果docs查出东西，下面就不执行了
        retriever2 = vs.as_retriever(search_kwargs={"k": 8})
        docs = retriever2.invoke(query)
        return {"docs": docs, "question": query, "debug": "fallback_unfiltered"}

    return {"docs": docs, "question": query, "debug": "filtered"}


def grade_evidence(state: QAState) -> str:
    """检索后判断是否有证据。"""
    return "good" if state.get("docs") else "bad"


def generate_answer(state: QAState) -> dict:
    """带引用生成答案。"""
    llm = get_llm()
    docs = state.get("docs", [])

    context = "\n\n".join(
        f"[{i+1}] {d.page_content}\n(source={d.metadata.get('source')}, page={d.metadata.get('page')})"
        for i, d in enumerate(docs[:6])
    )  # 将我检索出的内容拼接成一个大的字符串

    prompt = QA_USER.format(question=state["question"], context=context)
    messages = [
        AIMessage(content=QA_SYSTEM),
        HumanMessage(content=prompt)
    ]
    ans = llm.invoke(messages).content  # content表示大模型返回的结果
    return {"answer": ans}


def refuse_or_clarify(state: QAState) -> dict:
    """无证据兜底。"""
    return {
        "answer": "我没有在当前可见知识库中找到足够证据回答。请提供更具体的关键词/文档来源。"
    }


def build_qa_graph():
    g = StateGraph(QAState)

    # 注意：节点注册用 runnable（返回 dict）
    g.add_node("decide_retrieve", decide_retrieve_node)
    g.add_node("retrieve", retrieve)
    g.add_node("generate", generate_answer)
    g.add_node("refuse", refuse_or_clarify)

    # START -> decide_retrieve
    g.add_edge(START, "decide_retrieve")

    # decide_retrieve 的条件路由（用 decide_retrieve 条件函数）
    g.add_conditional_edges(
        "decide_retrieve",
        decide_retrieve,
        {
            "retrieve": "retrieve",
            "direct": "generate",
        },
    )

    # retrieve 后根据证据充分性路由
    g.add_conditional_edges(
        "retrieve",
        grade_evidence,
        {
            "good": "generate",
            "bad": "refuse",
        },
    )

    g.add_edge("generate", END)
    g.add_edge("refuse", END)

    return g.compile()


8. app/router_graph.py（顶层路由）
我们这里搭了一个顶层Router图，
这个非常简单，现在只有一种模式：不管怎样都把请求丢给qa_graph去做RAG。

后面我们可以在这里挂更多模式，比如：
• action -> 走前面那个工单创建workflow
• chat -> 走一个纯闲聊LLM
• tool -> 调用别的工具

现在是一个最小可用版本，只实现了Q&A路由
from typing import TypedDict, Any
from langgraph.graph import StateGraph, START, END
from app.rag.qa_graph import build_qa_graph

class RouterState(TypedDict, total=False):  # 顶层状态结构，total=False表示下面所有字段都是可选的
    question: str  # 给QA的问题
    text: str  # 用户原始文本
    user_role: str  # 用户角色
    mode: str  # 模式标记，比如qa，rag，kb等等
    answer: str  # 答案
    docs: list[Any]  # QA检索到的文档列表

def decide_route(state: RouterState) -> str:
	# 决定走哪个路由
    mode = (state.get("mode") or "").lower().strip()
    if mode in {"qa", "rag", "kb"}:
        return "qa"
    return "qa" # 这个代码反正最后都是qa


def route_node(state: RouterState) -> dict:
	# 路由前的站位节点，返回{}意味着不修改状态
    return {}


def build_router_graph():
    qa_graph = build_qa_graph()

    g = StateGraph(RouterState)

    g.add_node("route", route_node)
    g.add_node("qa", qa_graph)

    g.add_edge(START, "route")

    g.add_conditional_edges(
        "route",
        decide_route,
        {"qa": "qa"},
    )

    g.add_edge("qa", END)

    return g.compile()

router_graph = build_router_graph()
